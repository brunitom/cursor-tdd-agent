---
description:
  globs:
  alwaysApply: true
---

## Requirement Intake and Confirmation Flow

- When the user provides a requirement in any format, treat it as a new plan cycle:
  - **Natural language**: Direct requirement statements
  - **Markdown files**: Requirements docs, user stories, acceptance criteria
  - **Feature files**: Gherkin scenarios (.feature)
  - **JSON/XML**: API specs, config schemas, data contracts
  - **HTML**: Mock-ups, wireframes with behavior descriptions
- Switch to PLAN mode automatically for evaluation and test design.
- Do not generate any code or tests before explicit confirmation.

### Evaluate Requirement (PLAN)

#### 1. Memory Bank Consultation (Mandatory)
- Read `memory-bank/assessment.md` for current architecture, risks, and hotspots
- Check `memory-bank/riskMatrix.md` for prioritization guidance
- Review `memory-bank/testInventory.md` for existing test coverage
- Scan `memory-bank/specSources.md` for external requirements and constraints

#### 2. Requirement Analysis and Categorization
- **Parse format-specific content**:
  - `.feature` files: Extract Given/When/Then scenarios and map to acceptance tests
  - `.json/.xml`: Identify schemas, contracts, validation rules → contract/schema tests
  - `.md` files: Extract user stories, acceptance criteria → BDD scenarios
  - `.html` mockups: Identify UI interactions, accessibility requirements → UI/integration tests
- **Categorize requirement type**: [data validation, workflow change, API modification, UI enhancement, configuration, migration, etc.]
- **Map to testing techniques** per `testing-techniques.mdc` heuristics

#### 3. Spec Discovery and Integration
- Scan `test-specs/` and project root for related `.feature`, `.csv`, `.xml`, `.json` files
- Cross-reference with existing specs in `memory-bank/specSources.md`
- Identify dependencies and conflicts with existing requirements

#### 4. Test Matrix Proposal
- Restate the requirement succinctly and list assumptions/ambiguities to resolve
- Identify impacted domains/modules and critical invariants
- Propose a Test Matrix with priorities [Must/Should/Could]:
  - For each scenario include: intent, layer (unit/integration/contract), observable(s), technique, and rough test name
  - Reference specific memory bank files and existing specs
- Ask for explicit confirmation to proceed:
  - Say: "Confirm to proceed with tests-first implementation (reply ACT or CONFIRM TEST PLAN)."
  - If the user amends the plan, update the matrix and ask again

### Tests-First Execution (ACT)

#### 1. Micro-Cycle Test Generation (RED Phase)
- **For complex requirements**: Break into independent, granular failing tests
- **For simple requirements**: Generate all Must tests, then Should/Could incrementally
- Create/modify test files as per approved Test Matrix starting with Must priority
- If harness/setup is missing, scaffold minimum viable test runner/config
- Run tests; report failures (RED) with brief summary
- **Checkpoint**: Stop and await "Proceed" confirmation if large changes introduced

#### 2. Minimal Implementation (GREEN Phase)  
- **One test at a time**: Implement smallest possible change to make current failing test pass
- **Avoid gold-plating**: Don't implement features for tests not yet written
- Re-run tests after each micro-implementation
- Report GREEN status for each completed test
- **Only proceed** to next failing test when current is GREEN

#### 3. Refactor Phase (Optional)
- Propose safe refactors only after all Must tests are GREEN
- Ask for confirmation if non-trivial refactoring needed
- Keep tests GREEN; show diffs limited to refactor-only changes
- Re-run full test suite after refactoring

### Memory Bank Updates (Post-Implementation)
- **testInventory.md**: Add new tests (layer, coverage, ownership, flakiness notes)
- **riskMatrix.md**: Update if new risks discovered or mitigated
- **assessment.md**: Update if architecture or boundaries changed  
- **progress.md**: Mark requirement completed with test evidence
- **activeContext.md**: Update current focus and next priorities
- **testPlan.md**: Archive completed requirements, add follow-up tasks

### Guardrails
- Never implement production code before writing failing tests
- If a new test passes immediately, verify it is meaningful; strengthen assertions or adjust placement
- Keep diffs minimal and scoped to the current requirement
- Append a brief Self-Evaluation at the end of edits (see self-evaluation rule)

